{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_table(\"../smalldata/train_word.txt\",sep=\"\\n\",names=[\"fact\"])\n",
    "val=pd.read_table(\"../smalldata/val_word.txt\",sep=\"\\n\",names=[\"fact\"])\n",
    "test=pd.read_table(\"../smalldata/test_word.txt\",sep=\"\\n\",names=[\"fact\"])\n",
    "\n",
    "\n",
    "train_label=pd.read_table(\"../smalldata/train_label\",header=None,sep=\"\\n\")\n",
    "val_label=pd.read_table(\"../smalldata/val_label\",header=None,sep=\"\\n\")\n",
    "test_label=pd.read_table(\"../smalldata/test_label\",header=None,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154592, 1) (17131, 1) (32508, 1) (154592, 1) (17131, 1) (32508, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,val.shape,test.shape,train_label.shape,val_label.shape,test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1:transform the  text into idx\n",
      "step 1 run finish\n",
      "step 2: train the word2vec\n",
      "step 2 run finish\n",
      "step 3: transform into embedding matrix\n",
      "step 3 run finish\n",
      "(579810, 300) (154592, 400) 0.1985374519239061\n",
      "word to idx finish\n"
     ]
    }
   ],
   "source": [
    "def w2v_pad(train,val, test, maxlen_,victor_size):\n",
    "    max_features = 50000\n",
    "    count = 0\n",
    "    #第一步：将字或者词转化为id\n",
    "    print(\"step 1:transform the  text into idx\")\n",
    "   \n",
    "    tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "    tokenizer.fit_on_texts(pd.concat([train,val,test])['fact'].tolist())\n",
    "    train_ = sequence.pad_sequences(tokenizer.texts_to_sequences(train['fact'].tolist()), maxlen=maxlen_)\n",
    "    val_ = sequence.pad_sequences(tokenizer.texts_to_sequences(val['fact'].tolist()), maxlen=maxlen_)\n",
    "    test_ = sequence.pad_sequences(tokenizer.texts_to_sequences(test['fact'].tolist()), maxlen=maxlen_) \n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\"step 1 run finish\")\n",
    "        \n",
    "    #第二步：训练词向量\n",
    "    print(\"step 2: train the word2vec\")\n",
    "    file_name = '../cache/' + 'Word2Vec_' + str(victor_size) +'.model'\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"train word2vec\")\n",
    "        model = Word2Vec([line.split(\" \") for line in (pd.concat([train,val,test])['fact'].tolist())],\n",
    "                         size=victor_size, window=5, iter=15, workers=11, seed=2018, min_count=5)\n",
    "        model.save(file_name)\n",
    "    else:\n",
    "        model = Word2Vec.load(file_name)\n",
    "    print(\"step 2 run finish\")\n",
    "    \n",
    "    \n",
    "    #第三步：将其转化为matrix\n",
    "    print(\"step 3: transform into embedding matrix\")\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, victor_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = model[word] if word in model else None\n",
    "        if embedding_vector is not None:\n",
    "            count += 1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            unk_vec = np.random.random(victor_size) * 0.5\n",
    "            unk_vec = unk_vec - unk_vec.mean()\n",
    "            embedding_matrix[i] = unk_vec\n",
    "\n",
    "    print(\"step 3 run finish\")\n",
    "        \n",
    "    print(embedding_matrix.shape, train_.shape, count * 1.0 / embedding_matrix.shape[0]) \n",
    "    return train_,val_,test_, word_index, embedding_matrix\n",
    "\n",
    "word_seq_len=400\n",
    "victor_size=300\n",
    "\n",
    "train_,val_,test_, word2idx, word_embedding = w2v_pad(train,val,test, word_seq_len,victor_size)\n",
    "print(\"word to idx finish\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154592, 202) (32508, 202)\n"
     ]
    }
   ],
   "source": [
    "train_label[0]=train_label[0].map(lambda x:x.split(\" \"))\n",
    "val_label[0]=val_label[0].map(lambda x:x.split(\" \"))\n",
    "test_label[0]=test_label[0].map(lambda x:x.split(\" \"))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_label[0].tolist()+test_label[0].tolist()+val_label[0].tolist())\n",
    "\n",
    "train_label=mlb.transform(train_label[0].tolist())\n",
    "val_label=mlb.transform(val_label[0].tolist())\n",
    "test_label=mlb.transform(test_label[0].tolist())\n",
    "label_name = mlb.classes_\n",
    "\n",
    "print(train_label.shape,test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCNN(sent_length,embeddings_weight):\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "                            name=\"word_embedding\",\n",
    "                            input_dim=embeddings_weight.shape[0],\n",
    "                            weights=[embeddings_weight],\n",
    "                            output_dim=embeddings_weight.shape[1],\n",
    "                            trainable=False)\n",
    "    x=embedding(content)\n",
    "    conv1 = Conv1D(filters=64, kernel_size=1, padding='same')(x)\n",
    "    conv1 = MaxPool1D(pool_size=32)(conv1)\n",
    "    \n",
    "    \n",
    "    conv2 = Conv1D(filters=64, kernel_size=2, padding='same')(x)\n",
    "    conv2 = MaxPool1D(pool_size=32)(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "    conv3 = MaxPool1D(pool_size=32)(conv3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=64, kernel_size=4, padding='same')(x)\n",
    "    conv4 = MaxPool1D(pool_size=32)(conv4)\n",
    "    \n",
    "    cnn = concatenate([conv1, conv2, conv3, conv4], axis=-1)\n",
    "    fc = Flatten()(cnn)\n",
    "\n",
    "    #fc layer\n",
    "    fc=Dense(512)(fc)\n",
    "#     fc=BatchNormalization()(fc)\n",
    "    fc=Activation(activation=\"relu\")(fc)\n",
    "#     fc = Dropout(0.2)(fc)\n",
    "    \n",
    "    fc=Dense(256)(fc)\n",
    "    #fc=BatchNormalization()(fc)\n",
    "    fc = Activation(activation=\"relu\")(fc)\n",
    "    output = Dense(202,  activation=\"softmax\")(fc)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=content, outputs=output)\n",
    "    model.compile(loss= \"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 400, 300)     173943000   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 400, 64)      19264       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 400, 64)      38464       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 400, 64)      57664       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 400, 64)      76864       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 12, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 12, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 12, 64)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 12, 64)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12, 256)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3072)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1573376     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 202)          51914       activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 175,891,874\n",
      "Trainable params: 1,948,874\n",
      "Non-trainable params: 173,943,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 154592 samples, validate on 17131 samples\n",
      "Epoch 1/20\n",
      " - 32s - loss: 2.5675 - acc: 0.5186 - val_loss: 1.1968 - val_acc: 0.6976\n",
      "Epoch 2/20\n",
      " - 31s - loss: 1.5582 - acc: 0.6940 - val_loss: 1.0510 - val_acc: 0.7337\n",
      "Epoch 3/20\n",
      " - 31s - loss: 1.3956 - acc: 0.7251 - val_loss: 0.9766 - val_acc: 0.7561\n",
      "Epoch 4/20\n",
      " - 31s - loss: 1.3000 - acc: 0.7421 - val_loss: 0.9668 - val_acc: 0.7554\n",
      "Epoch 5/20\n",
      " - 31s - loss: 1.2293 - acc: 0.7549 - val_loss: 0.9545 - val_acc: 0.7656\n",
      "Epoch 6/20\n",
      " - 31s - loss: 1.1659 - acc: 0.7651 - val_loss: 0.9811 - val_acc: 0.7601\n",
      "Epoch 7/20\n",
      " - 31s - loss: 1.1206 - acc: 0.7736 - val_loss: 0.9912 - val_acc: 0.7568\n",
      "Epoch 8/20\n",
      " - 31s - loss: 1.0721 - acc: 0.7834 - val_loss: 1.0040 - val_acc: 0.7584\n",
      "Epoch 9/20\n",
      " - 31s - loss: 1.0430 - acc: 0.7866 - val_loss: 1.0245 - val_acc: 0.7546\n",
      "Epoch 10/20\n",
      " - 31s - loss: 1.0085 - acc: 0.7922 - val_loss: 1.0287 - val_acc: 0.7644\n",
      "Epoch 11/20\n",
      " - 31s - loss: 0.9755 - acc: 0.7997 - val_loss: 1.0434 - val_acc: 0.7667\n",
      "Epoch 12/20\n",
      " - 31s - loss: 0.9536 - acc: 0.8028 - val_loss: 1.0779 - val_acc: 0.7625\n",
      "Epoch 13/20\n",
      " - 31s - loss: 0.9274 - acc: 0.8077 - val_loss: 1.0662 - val_acc: 0.7593\n",
      "Epoch 14/20\n",
      " - 31s - loss: 0.9044 - acc: 0.8119 - val_loss: 1.1029 - val_acc: 0.7705\n",
      "Epoch 15/20\n",
      " - 31s - loss: 0.9006 - acc: 0.8129 - val_loss: 1.1115 - val_acc: 0.7613\n",
      "Epoch 16/20\n",
      " - 31s - loss: 0.8792 - acc: 0.8163 - val_loss: 1.1514 - val_acc: 0.7576\n",
      "Epoch 17/20\n",
      " - 31s - loss: 0.8638 - acc: 0.8182 - val_loss: 1.1479 - val_acc: 0.7636\n",
      "Epoch 18/20\n",
      " - 31s - loss: 0.8555 - acc: 0.8203 - val_loss: 1.1370 - val_acc: 0.7696\n",
      "Epoch 19/20\n",
      " - 31s - loss: 0.8434 - acc: 0.8220 - val_loss: 1.1845 - val_acc: 0.7582\n",
      "Epoch 20/20\n",
      " - 31s - loss: 0.8328 - acc: 0.8242 - val_loss: 1.1786 - val_acc: 0.7503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4f4376f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "file_path = \"../model/TextCNN.hdf\"\n",
    "\n",
    "model = TextCNN(word_seq_len, word_embedding)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=6)\n",
    "plateau = ReduceLROnPlateau(monitor=\"val_acc\", verbose=1, mode='max', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=False)\n",
    "#if not os.path.exists(file_path):\n",
    "model.fit(train_, train_label,\n",
    "              epochs=20,\n",
    "              batch_size=128,\n",
    "              validation_data=(val_, val_label),\n",
    "              #callbacks=[early_stopping, plateau, checkpoint],\n",
    "              verbose=2)\n",
    "# else:\n",
    "#     model.load_weights(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=model.predict(test_,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn Macro-F1-Score: 0.5554373854255992\n",
      "sklearn Macro-precision-Score: 0.6289731656984494\n",
      "sklearn Macro-recall-Score: 0.5299730109888571\n",
      "sklearn hamming_loss: 0.0026159592703456393\n"
     ]
    }
   ],
   "source": [
    "#仅预测1个label\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import *\n",
    "test_pred_onelabel=label_binarize(np.argmax(test_pred,axis=1),classes=list(range(0,202)))\n",
    "print('sklearn Macro-F1-Score:', f1_score(test_label, test_pred_onelabel, average='macro'))\n",
    "print('sklearn Macro-precision-Score:', precision_score(test_label, test_pred_onelabel, average='macro'))\n",
    "print('sklearn Macro-recall-Score:', recall_score(test_label, test_pred_onelabel, average='macro'))\n",
    "print('sklearn hamming_loss:', hamming_loss(test_label, test_pred_onelabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "sklearn Macro-F1-Score: 0.010407896254786374\n",
      "sklearn Macro-precision-Score: 0.005287350440470405\n",
      "sklearn Macro-recall-Score: 1.0\n",
      "sklearn hamming_loss: 0.9947126495595296\n",
      "0.05\n",
      "sklearn Macro-F1-Score: 0.5144346725699229\n",
      "sklearn Macro-precision-Score: 0.43670731440793775\n",
      "sklearn Macro-recall-Score: 0.6900723564290342\n",
      "sklearn hamming_loss: 0.005386640546668178\n",
      "0.1\n",
      "sklearn Macro-F1-Score: 0.5554054258904327\n",
      "sklearn Macro-precision-Score: 0.5098931264996042\n",
      "sklearn Macro-recall-Score: 0.6542859792928142\n",
      "sklearn hamming_loss: 0.0038505373239428042\n",
      "0.15000000000000002\n",
      "sklearn Macro-F1-Score: 0.571610897014695\n",
      "sklearn Macro-precision-Score: 0.5532853513895253\n",
      "sklearn Macro-recall-Score: 0.6283328974819034\n",
      "sklearn hamming_loss: 0.0032223598882590364\n",
      "0.2\n",
      "sklearn Macro-F1-Score: 0.5761391952984019\n",
      "sklearn Macro-precision-Score: 0.5809014556283769\n",
      "sklearn Macro-recall-Score: 0.6054050710455529\n",
      "sklearn hamming_loss: 0.002887331922561027\n",
      "0.25\n",
      "sklearn Macro-F1-Score: 0.5797097039336729\n",
      "sklearn Macro-precision-Score: 0.6074361284616707\n",
      "sklearn Macro-recall-Score: 0.5886154469071126\n",
      "sklearn hamming_loss: 0.00268418314699687\n",
      "0.30000000000000004\n",
      "sklearn Macro-F1-Score: 0.5771798432156464\n",
      "sklearn Macro-precision-Score: 0.6215385591987598\n",
      "sklearn Macro-recall-Score: 0.5702052420226028\n",
      "sklearn hamming_loss: 0.0025672279298804743\n",
      "0.35000000000000003\n",
      "sklearn Macro-F1-Score: 0.568575409192357\n",
      "sklearn Macro-precision-Score: 0.6317458650705322\n",
      "sklearn Macro-recall-Score: 0.5485184665540663\n",
      "sklearn hamming_loss: 0.002498699482351336\n",
      "0.4\n",
      "sklearn Macro-F1-Score: 0.5671974019786552\n",
      "sklearn Macro-precision-Score: 0.6515112128267819\n",
      "sklearn Macro-recall-Score: 0.5344357993022667\n",
      "sklearn hamming_loss: 0.0024718972450954954\n",
      "0.45\n",
      "sklearn Macro-F1-Score: 0.5577397673345604\n",
      "sklearn Macro-precision-Score: 0.6682589241301129\n",
      "sklearn Macro-recall-Score: 0.5159222150452262\n",
      "sklearn hamming_loss: 0.0024891054996972564\n",
      "0.5\n",
      "sklearn Macro-F1-Score: 0.5453895338340395\n",
      "sklearn Macro-precision-Score: 0.6669357700383469\n",
      "sklearn Macro-recall-Score: 0.4969160832737277\n",
      "sklearn hamming_loss: 0.0025255017196071767\n",
      "0.55\n",
      "sklearn Macro-F1-Score: 0.53501335279995\n",
      "sklearn Macro-precision-Score: 0.6780075136233795\n",
      "sklearn Macro-recall-Score: 0.4796162984596181\n",
      "sklearn hamming_loss: 0.0025896138894066596\n",
      "0.6000000000000001\n",
      "sklearn Macro-F1-Score: 0.5238740995431711\n",
      "sklearn Macro-precision-Score: 0.6797945559663356\n",
      "sklearn Macro-recall-Score: 0.46388740405137774\n",
      "sklearn hamming_loss: 0.0026616449020317313\n",
      "0.65\n",
      "sklearn Macro-F1-Score: 0.5112305582530124\n",
      "sklearn Macro-precision-Score: 0.6829867344170846\n",
      "sklearn Macro-recall-Score: 0.44727371051374276\n",
      "sklearn hamming_loss: 0.002722559077613188\n",
      "0.7000000000000001\n",
      "sklearn Macro-F1-Score: 0.4971380770062703\n",
      "sklearn Macro-precision-Score: 0.6845154393536712\n",
      "sklearn Macro-recall-Score: 0.42919270412661004\n",
      "sklearn hamming_loss: 0.0027772295501975446\n",
      "0.75\n",
      "sklearn Macro-F1-Score: 0.4878045023129802\n",
      "sklearn Macro-precision-Score: 0.6860507591502716\n",
      "sklearn Macro-recall-Score: 0.41363271490133896\n",
      "sklearn hamming_loss: 0.0028518494152848287\n",
      "0.8\n",
      "sklearn Macro-F1-Score: 0.47491985901886397\n",
      "sklearn Macro-precision-Score: 0.687245032405263\n",
      "sklearn Macro-recall-Score: 0.3963256610055329\n",
      "sklearn hamming_loss: 0.0029395658281221254\n",
      "0.8500000000000001\n",
      "sklearn Macro-F1-Score: 0.46010551338811884\n",
      "sklearn Macro-precision-Score: 0.6986568247096664\n",
      "sklearn Macro-recall-Score: 0.3766995483465471\n",
      "sklearn hamming_loss: 0.0030510387694361904\n",
      "0.9\n",
      "sklearn Macro-F1-Score: 0.43737922585964834\n",
      "sklearn Macro-precision-Score: 0.7009159195823822\n",
      "sklearn Macro-recall-Score: 0.35154448670039545\n",
      "sklearn hamming_loss: 0.003190227660639818\n",
      "0.9500000000000001\n",
      "sklearn Macro-F1-Score: 0.3972693505915991\n",
      "sklearn Macro-precision-Score: 0.6815773014480535\n",
      "sklearn Macro-recall-Score: 0.3099583875753521\n",
      "sklearn hamming_loss: 0.003418198962753418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "for i in np.arange(0,1,0.05):\n",
    "    print(i)\n",
    "    temp=test_pred.copy()\n",
    "    temp[temp<i]=0\n",
    "    temp[temp>=i]=1\n",
    "    print('sklearn Macro-F1-Score:', f1_score(test_label, temp, average='macro'))\n",
    "    print('sklearn Macro-precision-Score:', precision_score(test_label, temp, average='macro'))\n",
    "    print('sklearn Macro-recall-Score:', recall_score(test_label, temp, average='macro'))\n",
    "    print('sklearn hamming_loss:', hamming_loss(test_label, temp))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
